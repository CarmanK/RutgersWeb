<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width,initial-scale=1.0">
        <title>Kevin Carman</title>
        <link rel="shortcut icon" href="./pictures/k_favicon.png" type="image/x-icon">
        <link rel="stylesheet" type="text/css" href="./css/navbar_background.css">
        <link rel="stylesheet" type="text/css" href="./css/research.css">
    </head>
    <body>
        <div class="topnav">
            <p>Kevin Carman</p>
            <a href="./index.html">About Me</a>
            <a class="active" href="#">Research</a>
            <a href="./experience.html">Experience</a>
            <a href="./projects.html">Projects</a>
        </div>
        
        <div class="title">
            <p>DIMACS</p>
        </div>
        <div class="description">
            <p>My undergraduate research is being conducted as a part of the [<a href="http://reu.dimacs.rutgers.edu/">DIMACS</a>] (Discrete Mathematics & Theoretical Computer Science) REU program at Rutgers University during the summer of 2019 and is sponsored by the National Science Foundation.</p>
        </div>

        <div class="title">
            <p>Research Background</p>
        </div>
        <div class="description">
            <p>Dr. James Abello's area of research, my research supervisor, revolves around graph decomposition and data visualization. The peeling algorithm, developed by Dr. Abello, iteratively strips away the vertices of a graph based on their degree, with the final output being a partition of edges represented as a set of layers. Each of these layers consists of Abello fixed points. These fixed points are subgraphs such that when the peeling algorithm is applied to them, the result is the same as the input. This algorithm inspired Atlas. Atlas is an open-source project created by Dr. Abello and his team that runs in-browser and allows users to explore and interpret large graphs in their decomposed states. There is an extension of Atlas currently being worked on called Graph Waves. This extension applies a second algorithm to further decompose the larger fixed points that Atlas struggles with into waves.</p>
        </div>

        <div class="title">
            <p>Research Description</p>
        </div>
        <div class="description">
            <p>My research consists of using natural language processing techniques to analyze the metadata associated with the vertices of a given Abello fixed point to algorithmically develop a 'Graph Story' that summarizes the data and describes the relationships of the vertices. Other goals of this research include developing a medium to present the story to the user, optimizing the story writing process, and implementing the process into Atlas and Graph Waves.</p>
        </div>

        <div class="title">
            <p>Progress Log</p>
        </div>
        <div class="progress_log_content">
            <!-- Week Buttons -->
            <div class="tab">
                <button class="tablinks" onclick="openWeek(event, 'week1')">Week 1</button>
                <button class="tablinks" onclick="openWeek(event, 'week2')">Week 2</button>
                <button class="tablinks" onclick="openWeek(event, 'week3')">Week 3</button>
                <button class="tablinks" onclick="openWeek(event, 'week4')">Week 4</button>
                <button class="tablinks" onclick="openWeek(event, 'week5')">Week 5</button>
                <button class="tablinks" onclick="openWeek(event, 'week6')">Week 6</button>
                <button class="tablinks" onclick="openWeek(event, 'week7')">Week 7</button>
                <button class="tablinks" onclick="openWeek(event, 'week8')">Week 8</button>
                <button class="tablinks" onclick="openWeek(event, 'week9')">Week 9</button>
            </div>
            <!-- Week Content -->
            <div id="week1" class="tabcontent">
                <p>This week started with moving in and getting acquainted with the Rutgers campus and everyone involved with the DIMACS REU program. After orientation, I met with my supervisor and got an in-depth explanation of the current state of the project I will be working on. This really helped me understand what my project was about and how I will be contributing to it. I spent much of the week reading up on graph theory and other related research. I met the other two graduate students involved with this project on Friday and got to see the newest build of Atlas. Over the weekend, I was able to play around with the build and I documented my new user experience to aid the development of the project. We had our introductory presentations today, and it was interesting to learn about all of the other student's projects.</p>
            </div>
            <div id="week2" class="tabcontent">
                <p>I met with my mentor after the presentations to learn more about the algorithm we are using. This second part of the algorithm focuses on graph waves. He again explained the entire process very clearly. I mainly worked on analyzing small and common fixed point structures this week to start thinking of 'mini-stories'. On top of this, I was introduced to AutoPhrase. AutoPhrase is a program made to determine the important phrases in a given text with little to no human intervention. We talked about how we could incorporate the functionality of this program into my task of developing graph stories. One of the datasets that we are working with is a database of proteins. Each vertex in the graph is associated with a webpage containing information about it. We plan to look into ways to scrape the associated webpages for information, then use AutoPhrase to help us determine the useful information to eventually develop graph stories.</p>
            </div>
            <div id="week3" class="tabcontent">
                <p>This week started off with manually combing through the data associated with our protein graph to find connections and build a summary. The purpose of this was to get an idea of how to start doing it algorithmically and of how complex it can be. The next day, I had a conference call with my mentor, a graduate CS student involved in the project named Haodong, and a future Rutgers professor named Jingbo Shang. Jingbo discussed with us how his project, AutoPhrase, could be used to help create stories for the fixed points. AutoPhrase analyzes text and determines how words come together to create phrases, then ranks the phrases based on their contexual importance accordingly. Haodong and I then began working together to develop a way to algorithmically create these stories. I started off by writing a website scraper in Python to gather both the title and abstract associated with each protein. Haodong then ran the PubMed papers I gathered through an algorithm to calculate their TF-IDF scores and selected the top-k, 10 in this case, phrases. We then used a tool called Elasticsearch to index nearly 20,000 various PubMed abstracts. This was a very small subset of the PubMed abstracts we obtained because it was very slow to index them, and the more we had, the longer and more complex our querying then had to be. We also wanted to make sure that we had a working proof of concept first before diving into bigger datasets. After the abstracts were indexed, we queried every possible pair of phrases and kept the top-k, in this case 10 again, articles from the query to create a massive pool of relevant articles. Lastly, we iterated through each sentence in the pool and computed the TF-IDF scores along with how many new phrases the sentence covered until all pairs of phrases had been covered or the maximum number of sentences was reached. We have some work left to do, but from these results we hope to have generated accurate stories for these fixed points.</p>
            </div>
            <div id="week4" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week5" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week6" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week7" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week8" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week9" class="tabcontent">
                <p>TBD</p>
            </div>
    
            <script>
                function openWeek(event, weekName) {
                    var i, tabcontent, tablinks;
    
                    // Get all elements with class = "tabcontent" and hide them
                    tabcontent = document.getElementsByClassName("tabcontent");
                    for (i = 0; i < tabcontent.length; i++) {
                        tabcontent[i].style.display = "none";
                    }
    
                    // Get all elements with class = "tablinks" and remove the class "active"
                    tablinks = document.getElementsByClassName("tablinks");
                    for (i = 0; i < tablinks.length; i++) {
                        tablinks[i].className = tablinks[i].className.replace(" active", "");
                    }
    
                    // Show the current tab, and add an "active" class to the button that opened the tab
                    document.getElementById(weekName).style.display = "block";
                    event.currentTarget.className += " active";
                }
            </script>
        </div>

        <div class="title">
            <p>Deliverables</p>
        </div>
        <div class="description">
            <ul>
                <li>[<a href="https://github.com/CarmanK/GraphSemantics">GitHub Repository</a>]</li>
                <li>[<a href="./content/Graph Stories Introductory Presentation.pdf">Introductory Presentation</a>]</li>
                <!-- <li>[<a href="">Final Presentation</a>]</li> -->
                <!-- <li>[<a href="">Final Report</a></li>] -->
            </ul>
        </div>

        <div class="title">
            <p>References</p>
        </div>
        <div class="description">
            <ul>
                <li>[<a href="https://www.cs.rutgers.edu/faculty/james-abello-monedero">Dr. James Abello</a>]</li>
                <li>[<a href="https://fredhohman.com/papers/atlas">Atlas: Local Graph Exploration in a Global Context</a>]</li>
                <li>Graph Waves</li>
                <ul>
                    <li>Publication pending</li>
                </ul>
                <li>Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, "[<a href="https://arxiv.org/abs/1702.04457">Automated Phrase Mining from Massive Text Corpora</a>]", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.</li>
                <li>Jialu Liu, Jingbo Shang, Chi Wang, Xiang Ren and Jiawei Han, "[<a href="http://hanj.cs.illinois.edu/pdf/sigmod15_jliu.pdf">Mining Quality Phrases from Massive Text Corpora</a>]”, Proc. of 2015 ACM SIGMOD Int. Conf. on Management of Data (SIGMOD'15), Melbourne, Australia, May 2015.</li>
                <li>Kai Hong, Ani Nenkova, "[<a href="https://www.aclweb.org/anthology/E14-1075">Improving the Estimation of Word Importance for Newsx Multi-Document Summarization</a>]", Proc. of the 14th Conf. of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, April 2014.</li>
                <li>Josef Steinberger, Karel Ježek, "[<a href="http://www.cai.sk/ojs/index.php/cai/article/viewFile/37/24">Evaluation Measures for Text Summarization</a>]", Computing and Informatics, Vol. 28, March 2009.</li>
            </ul>
        </div>

        <div class="nsf">
            <p>Special thanks to Grant CCF-182215!</p>
        </div>
    </body>
</html>