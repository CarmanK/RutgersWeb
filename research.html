<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width,initial-scale=1.0">
        <title>Kevin Carman</title>
        <link rel="shortcut icon" href="./pictures/k_favicon.png" type="image/x-icon">
        <link rel="stylesheet" type="text/css" href="./css/navbar_background.css">
        <link rel="stylesheet" type="text/css" href="./css/research.css">
    </head>
    <body>
        <div class="topnav">
            <p>Kevin Carman</p>
            <a href="./index.html">About Me</a>
            <a class="active" href="#">Research</a>
            <a href="./experience.html">Experience</a>
            <a href="./projects.html">Projects</a>
        </div>
        
        <div class="title">
            <p>DIMACS</p>
        </div>
        <div class="description">
            <p>My undergraduate research is being conducted as a part of the [<a href="http://reu.dimacs.rutgers.edu/">DIMACS</a>] (Discrete Mathematics & Theoretical Computer Science) REU program at Rutgers University during the summer of 2019 and is sponsored by the National Science Foundation.</p>
        </div>

        <div class="title">
            <p>Research Background</p>
        </div>
        <div class="description">
            <p>Dr. James Abello's area of research, my research supervisor, revolves around graph decomposition and data visualization. The peeling algorithm, developed by Dr. Abello, iteratively strips away the vertices of a graph based on their degree, with the final output being a partition of edges represented as a set of layers. Each of these layers consists of Abello fixed points. These fixed points are subgraphs such that when the peeling algorithm is applied to them, the result is the same as the input. This algorithm inspired Atlas. Atlas is an open-source project created by Dr. Abello and his team that runs in-browser and allows users to explore and interpret large graphs in their decomposed states. There is an extension of Atlas currently being worked on called Graph Waves. This extension applies a second algorithm to further decompose the larger fixed points that Atlas struggles with into waves.</p>
        </div>

        <div class="title">
            <p>Research Description</p>
        </div>
        <div class="description">
            <p>My research consists of using natural language processing techniques to analyze the metadata associated with the vertices of a given Abello fixed point to algorithmically develop a 'Graph Story' that summarizes the data and describes the relationships of the vertices. Other goals of this research include developing a medium to present the story to the user, optimizing the story writing process, and implementing the process into Atlas and Graph Waves.</p>
        </div>

        <div class="title">
            <p>Progress Log (Work in Progress)</p>
        </div>
        <div class="progress_log_content">
            <!-- Week Buttons -->
            <div class="tab">
                <button class="tablinks" onclick="openWeek(event, 'week1')">Week 1</button>
                <button class="tablinks" onclick="openWeek(event, 'week2')">Week 2</button>
                <button class="tablinks" onclick="openWeek(event, 'week3')">Week 3</button>
                <button class="tablinks" onclick="openWeek(event, 'week4')">Week 4</button>
                <button class="tablinks" onclick="openWeek(event, 'week5')">Week 5</button>
                <button class="tablinks" onclick="openWeek(event, 'week6')">Week 6</button>
                <button class="tablinks" onclick="openWeek(event, 'week7')">Week 7</button>
                <button class="tablinks" onclick="openWeek(event, 'week8')">Week 8</button>
                <button class="tablinks" onclick="openWeek(event, 'week9')">Week 9</button>
            </div>
            <!-- Week Content -->
            <div id="week1" class="tabcontent">
                <p>This week consisted of many of the normal introductory type things you'd expect. I moved in and got acquainted with the Rutgers campus, my roommates, and everyone involved with the DIMACS REU program. We had orientation, then I met with my supervisor, Dr. James Abello, and he walked me through an in-depth explanation of what the project is and how I will be contributing to it. I learned how the peeling algorithm works and got to play around with the newest build of Atlas, called Graph Waves. Since I was the first user to test it out, I documented my experience and left plenty of feedback on its current state to aid them in development. I spent much of the rest of the week reading about various topics related to graph theory and information extraction.</p>
            </div>
            <div id="week2" class="tabcontent">
                <p>We started the week with our introductory presentations. It was interesting to learn about all of the other student's projects. I met with my mentor again to learn more about the other half of the algorithm he developed. This second part focused on what he called graph waves and how they were generated. I spent the next few days analyzing small and common fixed point structures. The goal of this was to start thinking of how we can take these fixed points and create graph stories from them. I was then introduced to Jingbo Shang, an expert in phrase mining, and his tool AutoPhrase. AutoPhrase is a tool designed to determine and tag the important phrases in a given text with little to no human intervention. We talked about how we could utilize the functionality of this tool to create graph stories, and laid out a plan to get started with development. We focused on the RCSB protein data bank as our dataset due to its complexity and a connection that Dr. Abello has with the people that use its data. In this case, we had an example fixed point where each vertex in the graph is associated with a protein accompanied by a webpage containing information about it.</p>
            </div>
            <div id="week3" class="tabcontent">
                <p>This week started off with manually combing through the data associated with our protein graph to find connections and build a summary. The purpose of this was to get an idea of how to start doing it algorithmically and of how complex it can be. The next day, I had a conference call with my mentor, a graduate CS student involved in the project named Haodong, and a future Rutgers professor named Jingbo Shang. Jingbo discussed with us how his project, AutoPhrase, could be used to help create stories for the fixed points. AutoPhrase analyzes text and determines how words come together to create phrases, then ranks the phrases based on their contexual importance accordingly. Haodong and I then began working together to develop a way to algorithmically create these stories. I started off by writing a website scraper in Python to gather both the title and abstract associated with each protein. Haodong then ran the PubMed papers I gathered through an algorithm to calculate their TF-IDF scores and selected the top-k, 10 in this case, phrases. We then used a tool called Elasticsearch to index nearly 20,000 various PubMed abstracts. This was a very small subset of the PubMed abstracts we obtained because it was very slow to index them, and the more we had, the longer and more complex our querying then had to be. We also wanted to make sure that we had a working proof of concept first before diving into bigger datasets. After the abstracts were indexed, we queried every possible pair of phrases and kept the top-k, in this case 10 again, articles from the query to create a massive pool of relevant articles. Lastly, we iterated through each sentence in the pool and computed the TF-IDF scores along with how many new phrases the sentence covered until all pairs of phrases had been covered or the maximum number of sentences was reached. We have some work left to do, but from these results we hope to have generated accurate stories for these fixed points.</p>
            </div>
            <div id="week4" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week5" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week6" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week7" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week8" class="tabcontent">
                <p>TBD</p>
            </div>
            <div id="week9" class="tabcontent">
                <p>TBD</p>
            </div>
    
            <script>
                function openWeek(event, weekName) {
                    var i, tabcontent, tablinks;
                    
                    // Get all elements with class = "tabcontent" and hide them
                    tabcontent = document.getElementsByClassName("tabcontent");
                    for(i = 0; i < tabcontent.length; i++){
                        tabcontent[i].style.display = "none";
                    }

                    if(event.currentTarget.className === "tablinks active"){
                        // Get all elements with class = "tablinks" and remove the class "active"
                        tablinks = document.getElementsByClassName("tablinks");
                        for (i = 0; i < tablinks.length; i++) {
                            tablinks[i].className = "tablinks";
                        }
                    }else{
                        // Get all elements with class = "tablinks" and remove the class "active"
                        tablinks = document.getElementsByClassName("tablinks");
                        for (i = 0; i < tablinks.length; i++) {
                            tablinks[i].className = "tablinks";
                        }
                        // Show the current tab, and add an "active" class to the button that opened the tab
                        document.getElementById(weekName).style.display = "block";
                        event.currentTarget.className += " active";
                    }
                }
            </script>
        </div>

        <div class="title">
            <p>Deliverables</p>
        </div>
        <div class="description">
            <ul>
                <li>[<a href="https://github.com/CarmanK/GraphSemantics">GitHub Repository</a>]</li>
                <li>[<a href="./content/Graph Stories Introductory Presentation.pdf">Introductory Presentation</a>]</li>
                <!-- <li>[<a href="">Final Presentation</a>]</li> -->
                <!-- <li>[<a href="">Final Report</a></li>] -->
            </ul>
        </div>

        <div class="title">
            <p>References</p>
        </div>
        <div class="description">
            <ul>
                <li>[<a href="https://www.cs.rutgers.edu/faculty/james-abello-monedero">Dr. James Abello</a>]</li>
                <li>[<a href="https://fredhohman.com/papers/atlas">Atlas: Local Graph Exploration in a Global Context</a>]</li>
                <li>Graph Waves</li>
                <ul>
                    <li>Publication pending</li>
                </ul>
                <li>Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, Jiawei Han, "[<a href="https://arxiv.org/abs/1702.04457">Automated Phrase Mining from Massive Text Corpora</a>]", accepted by IEEE Transactions on Knowledge and Data Engineering, Feb. 2018.</li>
                <li>Jialu Liu, Jingbo Shang, Chi Wang, Xiang Ren and Jiawei Han, "[<a href="http://hanj.cs.illinois.edu/pdf/sigmod15_jliu.pdf">Mining Quality Phrases from Massive Text Corpora</a>]”, Proc. of 2015 ACM SIGMOD Int. Conf. on Management of Data (SIGMOD'15), Melbourne, Australia, May 2015.</li>
                <li>Kai Hong, Ani Nenkova, "[<a href="https://www.aclweb.org/anthology/E14-1075">Improving the Estimation of Word Importance for Newsx Multi-Document Summarization</a>]", Proc. of the 14th Conf. of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, April 2014.</li>
                <li>Josef Steinberger, Karel Ježek, "[<a href="http://www.cai.sk/ojs/index.php/cai/article/viewFile/37/24">Evaluation Measures for Text Summarization</a>]", Computing and Informatics, Vol. 28, March 2009.</li>
            </ul>
        </div>

        <div class="nsf">
            <p>Special thanks to Grant CCF-182215!</p>
        </div>
    </body>
</html>